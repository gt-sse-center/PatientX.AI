{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from octis.preprocessing.preprocessing import Preprocessing\n",
    "from octis.models.LDA import LDA\n",
    "import os"
   ],
   "id": "a0baca0b46aacea5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"reading data...\")\n",
    "\n",
    "data_folder_path = os.path.join(os.getcwd(), \"forum-crawler-data\")\n",
    "\n",
    "# read data\n",
    "data_say_hello = pd.read_csv(os.path.join(data_folder_path, 'Say hello and introduce yourself.csv'))\n",
    "data_recently_diagnosed = pd.read_csv(os.path.join(data_folder_path, 'Recently diagnosed and early stages of dementia.csv'))\n",
    "data_memory_concerns = pd.read_csv(os.path.join(data_folder_path, 'Memory concerns and seeking a diagnosis.csv'))\n",
    "data_i_have_dementia = pd.read_csv(os.path.join(data_folder_path, 'I have dementia.csv'))\n",
    "data_i_have_partner = pd.read_csv(os.path.join(data_folder_path, 'I have a partner with dementia.csv'))\n",
    "data_i_care = pd.read_csv(os.path.join(data_folder_path, 'I care for a person with dementia.csv'))\n",
    "\n",
    "\n",
    "print(\"read data\")"
   ],
   "id": "a7d0e16fa9120daf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# combine data into single dataframe\n",
    "dfs = [data_say_hello, data_recently_diagnosed, data_memory_concerns, data_i_have_dementia, data_i_have_partner, data_i_care]\n",
    "forum_data_union = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "SAMPLE_SIZE = 50\n",
    "\n",
    "sample_data = forum_data_union.sample(SAMPLE_SIZE)\n",
    "\n",
    "print(\"sampled\")"
   ],
   "id": "95230b8ca0dc33d4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Save to TSV file\n",
    "\n",
    "sample_data.to_csv(path_or_buf='/Users/vnarayan35/Documents/GitHub/PatientX.AI/existing_code/dataset/sample_data_final_fixed.tsv', index=False, sep='\\t')"
   ],
   "id": "8530a7589e9266d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# group posts from the same forum/thread into one document and remove any line breaks\n",
    "data_union_grouped = forum_data_union.groupby(['forum', 'thread_title'], as_index=False).agg({'post_message': ''.join})\n",
    "data_union_grouped['post_message'] = data_union_grouped['post_message'].str.strip().replace(r'\\n', ' ', regex=True)"
   ],
   "id": "c854a0bd364aa344",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# shape of grouped data\n",
    "data_union_grouped.shape"
   ],
   "id": "13b42055a256f09f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# longest 'document' length\n",
    "data_union_grouped['post_message'].str.len().max()"
   ],
   "id": "b60ad16c766984ed",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# brief look at what the aggregated data looks like\n",
    "data_union_grouped.head(10)"
   ],
   "id": "1c61665159b83c47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "!pip install textblob",
   "id": "327a098f92a57ed5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from textblob import TextBlob  # or use autocorrect\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to remove non-English words and correct typos\n",
    "def clean_message(message):\n",
    "    # Process the message with SpaCy\n",
    "    doc = nlp(message)\n",
    "    \n",
    "    # Remove non-English words (based on SpaCy's 'lang' attribute)\n",
    "    filtered_tokens = [token.text for token in doc if token.lang_ == 'en']\n",
    "    \n",
    "    # Reconstruct the sentence from the filtered tokens\n",
    "    filtered_message = ' '.join(filtered_tokens)\n",
    "    \n",
    "    # Fix typos using TextBlob (or autocorrect)\n",
    "    corrected_message = str(TextBlob(filtered_message).correct())\n",
    "    \n",
    "    return corrected_message\n"
   ],
   "id": "3ba49583ece28645",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# sample 1000 documents from the data - makes for a more manageable dataset to work with\n",
    "trimmed = data_union_grouped.sample(1000)"
   ],
   "id": "41b422737beadc4d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# clean the sampled data by fixing typos and removing non-english words\n",
    "trimmed['cleaned_message'] = trimmed['post_message'].apply(clean_message)"
   ],
   "id": "5de21eff43623a40",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save this cleaned data to a text file where documents are separated by a line break\n",
    "trimmed['cleaned_message'].to_csv(r'./trimmed_cleaned.txt', header=None, index=None, sep='\\n', mode='a')"
   ],
   "id": "31b85e6ac5cb0aae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_union_grouped['post_message'] = data_union_grouped['post_message'].str.strip().replace(r'\\r', ' ', regex=True) # minor cleaning\n",
    "data_review_grouped = data_union_grouped['post_message'] # holds only the text from the dataset"
   ],
   "id": "cfe2ccf545538fe5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save posts to a txt file, documents separated by linebreaks\n",
    "data_review_grouped.to_csv(r'./corpus_threads_combined.txt', header=None, index=None, sep='\\n', mode='a')"
   ],
   "id": "92fcabcfa57755ab",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# First pass at preprocessing the entire dataset\n",
    "\n",
    "print(\"preprocessing...\")\n",
    "\n",
    "# preprocessing - remove whitespace, remove punctuation, convert to lowercase\n",
    "preprocessor = Preprocessing(vocabulary=None, max_features=None,\n",
    "                             remove_punctuation=True, punctuation=string.punctuation,\n",
    "                             lemmatize=True, stopword_list='english',\n",
    "                             min_chars=1, min_words_docs=0)\n",
    "\n",
    "dataset = preprocessor.preprocess_dataset(documents_path=r'./corpus.txt')\n",
    "print(\"done preprocessing\")"
   ],
   "id": "c5803f898b3691ba",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"saving...\")\n",
    "dataset.save(path='./processed_dataset/')\n",
    "print(\"done saving\")"
   ],
   "id": "6a4172eafe83421d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Topic modeling with OCTIS LDA\n",
    "\n",
    "model = LDA(num_topics=9)\n",
    "\n",
    "\n",
    "print(\"training lda...\")\n",
    "\n",
    "# Train the model using default partitioning choice\n",
    "output = model.train_model(dataset)\n",
    "\n",
    "print(\"done training\")\n",
    "\n",
    "print(*list(output.keys()), sep=\"\\n\") # Print the output identifiers\n",
    "\n",
    "for t in output['topics'][:5]:\n",
    "  print(\" \".join(t))"
   ],
   "id": "4928d66527eee061",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print results again\n",
    "\n",
    "for t in output['topics'][:20]:\n",
    "  print(\" \".join(t))"
   ],
   "id": "9e77dfe5a0ed50bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "# Initialize metric\n",
    "npmi = Coherence(texts=dataset.get_corpus(), topk=10, measure='c_npmi')\n",
    "# Initialize metric\n",
    "topic_diversity = TopicDiversity(topk=10)"
   ],
   "id": "eecebc3d0ab24ded",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Retrieve metrics score\n",
    "topic_diversity_score = topic_diversity.score(output)\n",
    "print(\"Topic diversity: \"+str(topic_diversity_score))\n",
    "\n",
    "npmi_score = npmi.score(output)\n",
    "print(\"Coherence: \"+str(npmi_score))"
   ],
   "id": "f6094faf8f3620a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# second preprocessor, changes in max doc freq and min doc freq\n",
    "print(\"preprocessing...\")\n",
    "\n",
    "# preprocessing - remove whitespace, remove punctuation, convert to lowercase\n",
    "preprocessor2 = Preprocessing(vocabulary=None, max_features=None,\n",
    "                             remove_punctuation=True, punctuation=string.punctuation,\n",
    "                             lemmatize=True, stopword_list='english',\n",
    "                             min_chars=1, min_words_docs=0, num_processes=10, min_df=0.0001, max_df=0.7)"
   ],
   "id": "c21991ea399957bc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# increase model max length\n",
    "preprocessor2.spacy_model.max_length = 100000000"
   ],
   "id": "947f5ad56b227472",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "dataset2 = preprocessor2.preprocess_dataset(documents_path=r'./corpus_threads_combined.txt')\n",
    "print(\"done preprocessing\")"
   ],
   "id": "92be11748305335d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"saving...\")\n",
    "dataset.save(path='./processed_dataset_2/')\n",
    "print(\"done saving\")"
   ],
   "id": "d8ce9fb8cc21e548",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# remove custom stop words that aren't caught by spacy's model\n",
    "from spacy.lang.en import stop_words\n",
    "\n",
    "stop_words = list(stop_words.STOP_WORDS)\n",
    "custom_stop_words = ['with', 'my', 'your', 'she', 'this', 'was', 'her', 'have', 'as', 'he', 'him', 'but', 'not', 'so', 'are', 'at', 'be', 'has', 'do', 'got', 'how', 'on', 'or', 'would', 'will', 'what', 'they', 'if', 'or', 'get', 'can', 'we', 'me', 'can', 'has', 'his', 'there', 'them', 'just', 'am', 'by', 'that', 'from', 'it', 'is', 'in', 'you', 'also', 'very', 'had', 'a', 'an', 'for']\n",
    "\n",
    "stop_words += custom_stop_words"
   ],
   "id": "4611233f48e0d48f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# third pass at preprocessing, changes in max df, min df, and using custom stop words\n",
    "preprocessor3 = Preprocessing(vocabulary=None, max_features=None,\n",
    "                             remove_punctuation=True, punctuation=string.punctuation,\n",
    "                             lemmatize=True, stopword_list=custom_stop_words,\n",
    "                             min_chars=1, min_words_docs=20, num_processes=10, min_df=0.01, max_df=0.5)"
   ],
   "id": "ec04200b458245c7",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "preprocessor3.spacy_model.max_length = 100000000",
   "id": "eb9be60e0e94826a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# check max doc length amongst the sampled datapoints\n",
    "trimmed['cleaned_message'].str.len().max()"
   ],
   "id": "36b35edbf959bfe2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# preprocess the sampled data\n",
    "dataset_trimmed_cleaned = preprocessor3.preprocess_dataset(documents_path=r'./trimmed_cleaned.txt')\n",
    "print(\"done preprocessing\")"
   ],
   "id": "3bf075cfd162e86c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# save dataset\n",
    "print(\"saving...\")\n",
    "dataset_trimmed_cleaned.save(path='./processed_dataset_trimmed/')\n",
    "print(\"done saving\")"
   ],
   "id": "56865d496066782",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# NOTE THIS IS A CUSTOM LDA MODEL THAT BUILDS OFF THE OCTIS LDA - THIS ONE USES TF-IDF INSTEAD OF BAG OF WORDS\n",
    "\n",
    "from octis.models.model import AbstractModel\n",
    "import numpy as np\n",
    "from gensim.models import ldamodel\n",
    "import gensim.corpora as corpora\n",
    "import octis.configuration.citations as citations\n",
    "import octis.configuration.defaults as defaults\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "\n",
    "class LDA(AbstractModel):\n",
    "\n",
    "    id2word = None\n",
    "    id_corpus = None\n",
    "    use_partitions = True\n",
    "    update_with_test = False\n",
    "\n",
    "    def __init__(\n",
    "        self, num_topics=100, distributed=False, chunksize=2000,\n",
    "        passes=1, update_every=1, alpha=\"symmetric\", eta=None, decay=0.5,\n",
    "        offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001,\n",
    "            random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize LDA model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_topics (int, optional) – The number of requested latent topics to\n",
    "        be extracted from the training corpus.\n",
    "\n",
    "        distributed (bool, optional) – Whether distributed computing should be\n",
    "        used to accelerate training.\n",
    "\n",
    "        chunksize (int, optional) – Number of documents to be used in each\n",
    "        training chunk.\n",
    "\n",
    "        passes (int, optional) – Number of passes through the corpus during\n",
    "        training.\n",
    "\n",
    "        update_every (int, optional) – Number of documents to be iterated\n",
    "        through for each update. Set to 0 for batch learning, > 1 for\n",
    "        online iterative learning.\n",
    "\n",
    "        alpha ({numpy.ndarray, str}, optional) – Can be set to an 1D array of\n",
    "        length equal to the number of expected topics that expresses our\n",
    "        a-priori belief for the each topics’ probability. Alternatively\n",
    "        default prior selecting strategies can be employed by supplying\n",
    "        a string:\n",
    "\n",
    "            ’asymmetric’: Uses a fixed normalized asymmetric prior of\n",
    "            1.0 / topicno.\n",
    "\n",
    "            ’auto’: Learns an asymmetric prior from the corpus\n",
    "            (not available if distributed==True).\n",
    "\n",
    "        eta ({float, np.array, str}, optional) – A-priori belief on word\n",
    "        probability, this can be:\n",
    "\n",
    "            scalar for a symmetric prior over topic/word probability,\n",
    "\n",
    "            vector of length num_words to denote an asymmetric user defined\n",
    "            probability for each word,\n",
    "\n",
    "            matrix of shape (num_topics, num_words) to assign a probability\n",
    "            for each word-topic combination,\n",
    "\n",
    "            the string ‘auto’ to learn the asymmetric prior from the data.\n",
    "\n",
    "        decay (float, optional) – A number between (0.5, 1] to weight what\n",
    "        percentage of the previous lambda value is forgotten when each new\n",
    "        document is examined.\n",
    "\n",
    "        offset (float, optional) – Hyper-parameter that controls how much\n",
    "        we will slow down the first steps the first few iterations.\n",
    "\n",
    "        eval_every (int, optional) – Log perplexity is estimated every\n",
    "        that many updates. Setting this to one slows down training by ~2x.\n",
    "\n",
    "        iterations (int, optional) – Maximum number of iterations through the\n",
    "        corpus when inferring the topic distribution of a corpus.\n",
    "\n",
    "        gamma_threshold (float, optional) – Minimum change in the value of the\n",
    "        gamma parameters to continue iterating.\n",
    "\n",
    "        random_state ({np.random.RandomState, int}, optional) – Either a\n",
    "        randomState object or a seed to generate one.s\n",
    "        Useful for reproducibility.\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hyperparameters = dict()\n",
    "        self.hyperparameters[\"num_topics\"] = num_topics\n",
    "        self.hyperparameters[\"distributed\"] = distributed\n",
    "        self.hyperparameters[\"chunksize\"] = chunksize\n",
    "        self.hyperparameters[\"passes\"] = passes\n",
    "        self.hyperparameters[\"update_every\"] = update_every\n",
    "        self.hyperparameters[\"alpha\"] = alpha\n",
    "        self.hyperparameters[\"eta\"] = eta\n",
    "        self.hyperparameters[\"decay\"] = decay\n",
    "        self.hyperparameters[\"offset\"] = offset\n",
    "        self.hyperparameters[\"eval_every\"] = eval_every\n",
    "        self.hyperparameters[\"iterations\"] = iterations\n",
    "        self.hyperparameters[\"gamma_threshold\"] = gamma_threshold\n",
    "        self.hyperparameters[\"random_state\"] = random_state\n",
    "\n",
    "    def info(self):\n",
    "        \"\"\"\n",
    "        Returns model informations\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"citation\": citations.models_LDA,\n",
    "            \"name\": \"LDA, Latent Dirichlet Allocation\"\n",
    "        }\n",
    "\n",
    "    def hyperparameters_info(self):\n",
    "        \"\"\"\n",
    "        Returns hyperparameters informations\n",
    "        \"\"\"\n",
    "        return defaults.LDA_hyperparameters_info\n",
    "\n",
    "    def set_hyperparameters(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Set model hyperparameters\n",
    "        \"\"\"\n",
    "        super().set_hyperparameters(**kwargs)\n",
    "        # Allow alpha to be a float in case of symmetric alpha\n",
    "        if \"alpha\" in kwargs:\n",
    "            if isinstance(kwargs[\"alpha\"], float):\n",
    "                self.hyperparameters[\"alpha\"] = [\n",
    "                    kwargs[\"alpha\"]\n",
    "                ] * self.hyperparameters[\"num_topics\"]\n",
    "\n",
    "    def partitioning(self, use_partitions, update_with_test=False):\n",
    "        \"\"\"\n",
    "        Handle the partitioning system to use and reset the model to perform\n",
    "        new evaluations\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        use_partitions: True if train/set partitioning is needed, False\n",
    "                        otherwise\n",
    "        update_with_test: True if the model should be updated with the test set,\n",
    "                          False otherwise\n",
    "        \"\"\"\n",
    "        self.use_partitions = use_partitions\n",
    "        self.update_with_test = update_with_test\n",
    "        self.id2word = None\n",
    "        self.id_corpus = None\n",
    "\n",
    "    def train_model(self, dataset, hyperparams=None, top_words=10):\n",
    "        \"\"\"\n",
    "        Train the model and return output\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : dataset to use to build the model\n",
    "        hyperparams : hyperparameters to build the model\n",
    "        top_words : if greater than 0 returns the most significant words for\n",
    "                    each topic in the output (Default True)\n",
    "        Returns\n",
    "        -------\n",
    "        result : dictionary with up to 3 entries,\n",
    "                 'topics', 'topic-word-matrix' and\n",
    "                 'topic-document-matrix'\n",
    "        \"\"\"\n",
    "        if hyperparams is None:\n",
    "            hyperparams = {}\n",
    "\n",
    "        if self.use_partitions:\n",
    "            train_corpus, test_corpus = dataset.get_partitioned_corpus(\n",
    "                use_validation=False)\n",
    "        else:\n",
    "            train_corpus = dataset.get_corpus()\n",
    "\n",
    "        if self.id2word is None:\n",
    "            self.id2word = corpora.Dictionary(dataset.get_corpus())\n",
    "\n",
    "        if self.id_corpus is None:\n",
    "            self.id_corpus = [self.id2word.doc2bow(document)\n",
    "                              for document in train_corpus]\n",
    "\n",
    "        if \"num_topics\" not in hyperparams:\n",
    "            hyperparams[\"num_topics\"] = self.hyperparameters[\"num_topics\"]\n",
    "\n",
    "        # Allow alpha to be a float in case of symmetric alpha\n",
    "        if \"alpha\" in hyperparams:\n",
    "            if isinstance(hyperparams[\"alpha\"], float):\n",
    "                hyperparams[\"alpha\"] = [\n",
    "                    hyperparams[\"alpha\"]\n",
    "                ] * hyperparams[\"num_topics\"]\n",
    "\n",
    "        #### changes #####\n",
    "        print(\"using tf-idf\")\n",
    "        tfidf_model = TfidfModel(self.id_corpus)\n",
    "        tfidf_corpus = tfidf_model[self.id_corpus]\n",
    "        hyperparams[\"corpus\"] = tfidf_corpus\n",
    "        \n",
    "        \n",
    "        # hyperparams[\"corpus\"] = self.id_corpus\n",
    "        \n",
    "        hyperparams[\"id2word\"] = self.id2word\n",
    "        self.hyperparameters.update(hyperparams)\n",
    "\n",
    "        self.trained_model = ldamodel.LdaModel(**self.hyperparameters)\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        result[\"topic-word-matrix\"] = self.trained_model.get_topics()\n",
    "\n",
    "        if top_words > 0:\n",
    "            topics_output = []\n",
    "            for topic in result[\"topic-word-matrix\"]:\n",
    "                top_k = np.argsort(topic)[-top_words:]\n",
    "                top_k_words = list(reversed([self.id2word[i] for i in top_k]))\n",
    "                topics_output.append(top_k_words)\n",
    "            result[\"topics\"] = topics_output\n",
    "\n",
    "        result[\"topic-document-matrix\"] = self._get_topic_document_matrix()\n",
    "\n",
    "        if self.use_partitions:\n",
    "            new_corpus = [self.id2word.doc2bow(\n",
    "                document) for document in test_corpus]\n",
    "            if self.update_with_test:\n",
    "                self.trained_model.update(new_corpus)\n",
    "                self.id_corpus.extend(new_corpus)\n",
    "\n",
    "                result[\"test-topic-word-matrix\"] = (\n",
    "                    self.trained_model.get_topics())\n",
    "\n",
    "                if top_words > 0:\n",
    "                    topics_output = []\n",
    "                    for topic in result[\"test-topic-word-matrix\"]:\n",
    "                        top_k = np.argsort(topic)[-top_words:]\n",
    "                        top_k_words = list(\n",
    "                            reversed([self.id2word[i] for i in top_k]))\n",
    "                        topics_output.append(top_k_words)\n",
    "                    result[\"test-topics\"] = topics_output\n",
    "\n",
    "                result[\"test-topic-document-matrix\"] = (\n",
    "                    self._get_topic_document_matrix())\n",
    "\n",
    "            else:\n",
    "                test_document_topic_matrix = []\n",
    "                for document in new_corpus:\n",
    "                    document_topics_tuples = self.trained_model[document]\n",
    "                    document_topics = np.zeros(\n",
    "                        self.hyperparameters[\"num_topics\"])\n",
    "                    for single_tuple in document_topics_tuples:\n",
    "                        document_topics[single_tuple[0]] = single_tuple[1]\n",
    "\n",
    "                    test_document_topic_matrix.append(document_topics)\n",
    "                result[\"test-topic-document-matrix\"] = np.array(\n",
    "                    test_document_topic_matrix).transpose()\n",
    "        return result\n",
    "\n",
    "    def _get_topics_words(self, topk):\n",
    "        \"\"\"\n",
    "        Return the most significative words for each topic.\n",
    "        \"\"\"\n",
    "        topic_terms = []\n",
    "        for i in range(self.hyperparameters[\"num_topics\"]):\n",
    "            topic_words_list = []\n",
    "            for word_tuple in self.trained_model.get_topic_terms(i, topk):\n",
    "                topic_words_list.append(self.id2word[word_tuple[0]])\n",
    "            topic_terms.append(topic_words_list)\n",
    "        return topic_terms\n",
    "\n",
    "    def _get_topic_document_matrix(self):\n",
    "        \"\"\"\n",
    "        Return the topic representation of the\n",
    "        corpus\n",
    "        \"\"\"\n",
    "        doc_topic_tuples = []\n",
    "        for document in self.id_corpus:\n",
    "            doc_topic_tuples.append(\n",
    "                self.trained_model.get_document_topics(document,\n",
    "                                                       minimum_probability=0))\n",
    "\n",
    "        topic_document = np.zeros((\n",
    "            self.hyperparameters[\"num_topics\"],\n",
    "            len(doc_topic_tuples)))\n",
    "\n",
    "        for ndoc in range(len(doc_topic_tuples)):\n",
    "            document = doc_topic_tuples[ndoc]\n",
    "            for topic_tuple in document:\n",
    "                topic_document[topic_tuple[0]][ndoc] = topic_tuple[1]\n",
    "        return topic_document"
   ],
   "id": "36477ea42ae04637",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# train custom model on sampled data\n",
    "\n",
    "model_trim = LDA(num_topics=6)\n",
    "\n",
    "\n",
    "print(\"training lda...\")\n",
    "\n",
    "# Train the model using default partitioning choice\n",
    "output = model_trim.train_model(dataset_trimmed_cleaned, top_words=50)\n",
    "\n",
    "print(\"done training\")\n",
    "\n",
    "print(*list(output.keys()), sep=\"\\n\") # Print the output identifiers\n",
    "\n",
    "for t in output['topics'][:20]:\n",
    "  print(\" \".join(t))"
   ],
   "id": "4da160ebb7ad3f9c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print results\n",
    "for t in output['topics']:\n",
    "  print(\" \".join(t))\n",
    "  print(\"\\n\")"
   ],
   "id": "4209ac35d9babf71",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TWO FUNCTIONS FOR POST-PROCESSING\n",
    "\n",
    "from typing import List\n",
    "\n",
    "# take the outputted lists and return only words that are unique to their respective lists\n",
    "def unique_words(lists: List[List[str]]) -> List[List[str]]:\n",
    "    # Convert each sublist to a set for easier manipulation\n",
    "    sets = [set(sublist) for sublist in lists]\n",
    "    unique_lists = []\n",
    "\n",
    "    for i, word_set in enumerate(sets):\n",
    "        # Calculate the union of all sets except the current one\n",
    "        other_words = set().union(*[s for j, s in enumerate(sets) if j != i])\n",
    "        # Find words unique to the current set\n",
    "        unique_lists.append(list(word_set - other_words))\n",
    "\n",
    "    return unique_lists\n",
    "\n",
    "\n",
    "# take outputted words and keep only words that show up in less than <threshold> percent of topics\n",
    "def unique_words_threshold(lists: List[List[str]], threshold: float) -> List[List[str]]:\n",
    "     # Total number of lists\n",
    "    num_lists = len(lists)\n",
    "    # Calculate the threshold count based on the percentage\n",
    "    max_allowed_count = int(threshold * num_lists)\n",
    "     \n",
    "     \n",
    "    # Convert each sublist to a set for easier manipulation\n",
    "    sets = [set(sublist) for sublist in lists]\n",
    "    unique_lists = []\n",
    "    \n",
    "    for i, word_set in enumerate(sets):\n",
    "        new_set = []\n",
    "        for word in word_set:\n",
    "            other_counts = 0\n",
    "            for j, other_set in enumerate(sets):\n",
    "                if word in other_set and i != j:\n",
    "                    other_counts += 1\n",
    "        \n",
    "            if other_counts <= max_allowed_count:\n",
    "                new_set.append(word)\n",
    "        unique_lists.append(new_set)\n",
    "\n",
    "    return unique_lists"
   ],
   "id": "9f3cf7529dbc545f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print unique words\n",
    "for l in unique_words(output['topics']):\n",
    "    print(\" \".join(l))\n",
    "    print(\"\\n\")"
   ],
   "id": "e5e7f6fc62606339",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# print 20 percent threshold results\n",
    "unique_threshold_20 = unique_words_threshold(output['topics'], 0.2)\n",
    "\n",
    "for l in unique_threshold_20:\n",
    "    print(\" \".join(l))\n",
    "    print(\"\\n\")"
   ],
   "id": "7a068f7d913f258e",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
