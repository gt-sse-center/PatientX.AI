{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T16:04:14.379554Z",
     "start_time": "2024-11-13T16:04:10.865746Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "from octis.preprocessing.preprocessing import Preprocessing\n",
    "from octis.models.LDA import LDA"
   ],
   "id": "a0baca0b46aacea5",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/patientx/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T16:04:21.377204Z",
     "start_time": "2024-11-13T16:04:18.524144Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"reading data...\")\n",
    "\n",
    "# read data\n",
    "data_say_hello = pd.read_csv('/Users/vnarayan35/Documents/GitHub/PatientX.AI/existing_code/forum-crawler data/Say hello and introduce yourself.csv')\n",
    "data_recently_diagnosed = pd.read_csv('/Users/vnarayan35/Documents/GitHub/PatientX.AI/existing_code/forum-crawler data/Recently diagnosed and early stages of dementia.csv')\n",
    "data_memory_concerns = pd.read_csv(\"/Users/vnarayan35/Documents/GitHub/PatientX.AI/existing_code/forum-crawler data/Memory concerns and seeking a diagnosis.csv\")\n",
    "data_i_have_dementia = pd.read_csv(\"/Users/vnarayan35/Documents/GitHub/PatientX.AI/existing_code/forum-crawler data/I have dementia.csv\")\n",
    "data_i_have_partner = pd.read_csv(\"/Users/vnarayan35/Documents/GitHub/PatientX.AI/existing_code/forum-crawler data/I have a partner with dementia.csv\")\n",
    "data_i_care = pd.read_csv(\"/Users/vnarayan35/Documents/GitHub/PatientX.AI/existing_code/forum-crawler data/I care for a person with dementia.csv\")\n",
    "\n",
    "\n",
    "print(\"read data\")"
   ],
   "id": "a7d0e16fa9120daf",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading data...\n",
      "read data\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T16:04:28.865456Z",
     "start_time": "2024-11-13T16:04:28.828411Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# combine data into single dataframe\n",
    "dfs = [data_say_hello, data_recently_diagnosed, data_memory_concerns, data_i_have_dementia, data_i_have_partner, data_i_care]\n",
    "forum_data_union = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "\n",
    "SAMPLE_SIZE = 50\n",
    "\n",
    "sample_data = forum_data_union.sample(SAMPLE_SIZE)\n",
    "\n",
    "print(\"sampled\")"
   ],
   "id": "95230b8ca0dc33d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T18:35:05.190737Z",
     "start_time": "2024-11-12T18:35:02.678272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Save to TSV file\n",
    "\n",
    "sample_data.to_csv(path_or_buf='/Users/vnarayan35/Documents/GitHub/PatientX.AI/existing_code/dataset/sample_data_final_fixed.tsv', index=False, sep='\\t')"
   ],
   "id": "8530a7589e9266d1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T16:04:39.443930Z",
     "start_time": "2024-11-13T16:04:39.012763Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# group posts from the same forum/thread into one document and remove any line breaks\n",
    "data_union_grouped = forum_data_union.groupby(['forum', 'thread_title'], as_index=False).agg({'post_message': ''.join})\n",
    "data_union_grouped['post_message'] = data_union_grouped['post_message'].str.strip().replace(r'\\n', ' ', regex=True)"
   ],
   "id": "c854a0bd364aa344",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T16:06:01.411722Z",
     "start_time": "2024-11-13T16:06:01.409194Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# shape of grouped data\n",
    "data_union_grouped.shape"
   ],
   "id": "13b42055a256f09f",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(42020, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T16:05:26.196980Z",
     "start_time": "2024-11-13T16:05:26.182487Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# longest 'document' length\n",
    "data_union_grouped['post_message'].str.len().max()"
   ],
   "id": "b60ad16c766984ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11678053"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T19:03:39.845214Z",
     "start_time": "2024-11-12T19:03:39.840128Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# brief look at what the aggregated data looks like\n",
    "data_union_grouped.head(10)"
   ],
   "id": "1c61665159b83c47",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                               forum  \\\n",
       "0  I care for a person with dementia   \n",
       "1  I care for a person with dementia   \n",
       "2  I care for a person with dementia   \n",
       "3  I care for a person with dementia   \n",
       "4  I care for a person with dementia   \n",
       "5  I care for a person with dementia   \n",
       "6  I care for a person with dementia   \n",
       "7  I care for a person with dementia   \n",
       "8  I care for a person with dementia   \n",
       "9  I care for a person with dementia   \n",
       "\n",
       "                                        thread_title  \\\n",
       "0  !Advice on dealing with delusions/regression -...   \n",
       "1                              \"Continence\" problems   \n",
       "2                             \"Covering up\" mistakes   \n",
       "3  \"Do I like her\" Mum asked me about her new car...   \n",
       "4        \"Dont ever discuss hallucinations,Distract\"   \n",
       "5            \"Fight\" in My Mums's Nursing Home Today   \n",
       "6                 \"Getting someone into a care home\"   \n",
       "7                                \"Going Home\" again.   \n",
       "8                      \"I don't want to be a burden\"   \n",
       "9                 \"I don't want to be here any more\"   \n",
       "\n",
       "                                        post_message  \n",
       "0  Hi everyone. This is my first post.\\rI would l...  \n",
       "1  Just wondering if anyone's experienced this an...  \n",
       "2  Like if she gets something wrong or says somet...  \n",
       "3  Well day two was a disaster. \\rCan you believe...  \n",
       "4  This is what the consultant said last week. No...  \n",
       "5  While sitting with my mum today in the lounge ...  \n",
       "6  That probably isn't a good title to a post but...  \n",
       "7  Several years ago my father went through the p...  \n",
       "8  My father periodically says he needs to buy ne...  \n",
       "9  My father told his carer this evening that he ...  "
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>forum</th>\n",
       "      <th>thread_title</th>\n",
       "      <th>post_message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I care for a person with dementia</td>\n",
       "      <td>!Advice on dealing with delusions/regression -...</td>\n",
       "      <td>Hi everyone. This is my first post.\\rI would l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I care for a person with dementia</td>\n",
       "      <td>\"Continence\" problems</td>\n",
       "      <td>Just wondering if anyone's experienced this an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I care for a person with dementia</td>\n",
       "      <td>\"Covering up\" mistakes</td>\n",
       "      <td>Like if she gets something wrong or says somet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I care for a person with dementia</td>\n",
       "      <td>\"Do I like her\" Mum asked me about her new car...</td>\n",
       "      <td>Well day two was a disaster. \\rCan you believe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I care for a person with dementia</td>\n",
       "      <td>\"Dont ever discuss hallucinations,Distract\"</td>\n",
       "      <td>This is what the consultant said last week. No...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>I care for a person with dementia</td>\n",
       "      <td>\"Fight\" in My Mums's Nursing Home Today</td>\n",
       "      <td>While sitting with my mum today in the lounge ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I care for a person with dementia</td>\n",
       "      <td>\"Getting someone into a care home\"</td>\n",
       "      <td>That probably isn't a good title to a post but...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I care for a person with dementia</td>\n",
       "      <td>\"Going Home\" again.</td>\n",
       "      <td>Several years ago my father went through the p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>I care for a person with dementia</td>\n",
       "      <td>\"I don't want to be a burden\"</td>\n",
       "      <td>My father periodically says he needs to buy ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>I care for a person with dementia</td>\n",
       "      <td>\"I don't want to be here any more\"</td>\n",
       "      <td>My father told his carer this evening that he ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T16:33:51.798125Z",
     "start_time": "2024-11-13T16:33:50.689369Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install textblob",
   "id": "327a098f92a57ed5",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\r\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\r\n",
      "Requirement already satisfied: nltk>=3.8 in /opt/anaconda3/envs/patientx/lib/python3.10/site-packages (from textblob) (3.9.1)\r\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/patientx/lib/python3.10/site-packages (from nltk>=3.8->textblob) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/patientx/lib/python3.10/site-packages (from nltk>=3.8->textblob) (1.4.2)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/patientx/lib/python3.10/site-packages (from nltk>=3.8->textblob) (2024.11.6)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/patientx/lib/python3.10/site-packages (from nltk>=3.8->textblob) (4.67.0)\r\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\r\n",
      "\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m626.3/626.3 kB\u001B[0m \u001B[31m18.8 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0m\r\n",
      "\u001B[?25hInstalling collected packages: textblob\r\n",
      "Successfully installed textblob-0.18.0.post0\r\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T16:34:49.207055Z",
     "start_time": "2024-11-13T16:34:48.324941Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import spacy\n",
    "from textblob import TextBlob  # or use autocorrect\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Function to remove non-English words and correct typos\n",
    "def clean_message(message):\n",
    "    # Process the message with SpaCy\n",
    "    doc = nlp(message)\n",
    "    \n",
    "    # Remove non-English words (based on SpaCy's 'lang' attribute)\n",
    "    filtered_tokens = [token.text for token in doc if token.lang_ == 'en']\n",
    "    \n",
    "    # Reconstruct the sentence from the filtered tokens\n",
    "    filtered_message = ' '.join(filtered_tokens)\n",
    "    \n",
    "    # Fix typos using TextBlob (or autocorrect)\n",
    "    corrected_message = str(TextBlob(filtered_message).correct())\n",
    "    \n",
    "    return corrected_message\n"
   ],
   "id": "3ba49583ece28645",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T17:52:10.433529Z",
     "start_time": "2024-11-13T17:52:10.430282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# sample 1000 documents from the data - makes for a more manageable dataset to work with\n",
    "trimmed = data_union_grouped.sample(1000)"
   ],
   "id": "41b422737beadc4d",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T18:22:44.555814Z",
     "start_time": "2024-11-13T17:52:53.096443Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# clean the sampled data by fixing typos and removing non-english words\n",
    "trimmed['cleaned_message'] = trimmed['post_message'].apply(clean_message)"
   ],
   "id": "5de21eff43623a40",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T18:28:08.446219Z",
     "start_time": "2024-11-13T18:28:08.384338Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save this cleaned data to a text file where documents are separated by a line break\n",
    "trimmed['cleaned_message'].to_csv(r'./trimmed_cleaned.txt', header=None, index=None, sep='\\n', mode='a')"
   ],
   "id": "31b85e6ac5cb0aae",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T19:36:53.601709Z",
     "start_time": "2024-11-12T19:36:51.400440Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_union_grouped['post_message'] = data_union_grouped['post_message'].str.strip().replace(r'\\r', ' ', regex=True) # minor cleaning\n",
    "data_review_grouped = data_union_grouped['post_message'] # holds only the text from the dataset"
   ],
   "id": "cfe2ccf545538fe5",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T19:37:33.299173Z",
     "start_time": "2024-11-12T19:37:31.156211Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save posts to a txt file, documents separated by linebreaks\n",
    "data_review_grouped.to_csv(r'./corpus_threads_combined.txt', header=None, index=None, sep='\\n', mode='a')"
   ],
   "id": "92fcabcfa57755ab",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T16:00:17.976066Z",
     "start_time": "2024-11-11T18:35:47.675082Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12118887/12118887 [21:23:34<00:00, 157.36it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created vocab\n",
      "116455\n",
      "done preprocessing\n"
     ]
    }
   ],
   "execution_count": 6,
   "source": [
    "# First pass at preprocessing the entire dataset\n",
    "\n",
    "print(\"preprocessing...\")\n",
    "\n",
    "# preprocessing - remove whitespace, remove punctuation, convert to lowercase\n",
    "preprocessor = Preprocessing(vocabulary=None, max_features=None,\n",
    "                             remove_punctuation=True, punctuation=string.punctuation,\n",
    "                             lemmatize=True, stopword_list='english',\n",
    "                             min_chars=1, min_words_docs=0)\n",
    "\n",
    "dataset = preprocessor.preprocess_dataset(documents_path=r'./corpus.txt')\n",
    "print(\"done preprocessing\")"
   ],
   "id": "c5803f898b3691ba"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T17:30:45.541344Z",
     "start_time": "2024-11-12T17:30:35.629651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"saving...\")\n",
    "dataset.save(path='./processed_dataset/')\n",
    "print(\"done saving\")"
   ],
   "id": "6a4172eafe83421d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving...\n",
      "done saving\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T17:46:22.427281Z",
     "start_time": "2024-11-12T17:38:11.087150Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Topic modeling with OCTIS LDA\n",
    "\n",
    "model = LDA(num_topics=9)\n",
    "\n",
    "\n",
    "print(\"training lda...\")\n",
    "\n",
    "# Train the model using default partitioning choice\n",
    "output = model.train_model(dataset)\n",
    "\n",
    "print(\"done training\")\n",
    "\n",
    "print(*list(output.keys()), sep=\"\\n\") # Print the output identifiers\n",
    "\n",
    "for t in output['topics'][:5]:\n",
    "  print(\" \".join(t))"
   ],
   "id": "4928d66527eee061",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training lda...\n",
      "done training\n",
      "topic-word-matrix\n",
      "topics\n",
      "topic-document-matrix\n",
      "test-topic-document-matrix\n",
      "pay money house use work buy drive council cost key\n",
      "hi uk www org alzheimers https attorney power play http\n",
      "post hope sorry help find read support hello advice welcome\n",
      "know think feel time thing mum like want try year\n",
      "thank good x love send point wish hope xx hug\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T17:57:40.453792Z",
     "start_time": "2024-11-12T17:57:40.451693Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print results again\n",
    "\n",
    "for t in output['topics'][:20]:\n",
    "  print(\" \".join(t))"
   ],
   "id": "9e77dfe5a0ed50bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pay money house use work buy drive council cost key\n",
      "hi uk www org alzheimers https attorney power play http\n",
      "post hope sorry help find read support hello advice welcome\n",
      "know think feel time thing mum like want try year\n",
      "thank good x love send point wish hope xx hug\n",
      "dementia year memory gp problem doctor mum medication ago diagnose\n",
      "not s m good i t luck look write d\n",
      "day go mum time get say night come week bed\n",
      "care home need mum carer help social dad hospital service\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T17:59:59.609413Z",
     "start_time": "2024-11-12T17:59:28.237182Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from octis.evaluation_metrics.diversity_metrics import TopicDiversity\n",
    "from octis.evaluation_metrics.coherence_metrics import Coherence\n",
    "# Initialize metric\n",
    "npmi = Coherence(texts=dataset.get_corpus(), topk=10, measure='c_npmi')\n",
    "# Initialize metric\n",
    "topic_diversity = TopicDiversity(topk=10)"
   ],
   "id": "eecebc3d0ab24ded",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T18:03:04.262170Z",
     "start_time": "2024-11-12T18:00:10.669954Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Retrieve metrics score\n",
    "topic_diversity_score = topic_diversity.score(output)\n",
    "print(\"Topic diversity: \"+str(topic_diversity_score))\n",
    "\n",
    "npmi_score = npmi.score(output)\n",
    "print(\"Coherence: \"+str(npmi_score))"
   ],
   "id": "f6094faf8f3620a",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic diversity: 0.9111111111111111\n",
      "Coherence: 0.08240201214883959\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T18:28:44.250639Z",
     "start_time": "2024-11-13T18:28:44.063415Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# second preprocessor, changes in max doc freq and min doc freq\n",
    "print(\"preprocessing...\")\n",
    "\n",
    "# preprocessing - remove whitespace, remove punctuation, convert to lowercase\n",
    "preprocessor2 = Preprocessing(vocabulary=None, max_features=None,\n",
    "                             remove_punctuation=True, punctuation=string.punctuation,\n",
    "                             lemmatize=True, stopword_list='english',\n",
    "                             min_chars=1, min_words_docs=0, num_processes=10, min_df=0.0001, max_df=0.7)"
   ],
   "id": "c21991ea399957bc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preprocessing...\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T21:58:01.861662Z",
     "start_time": "2024-11-12T21:58:01.859683Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# increase model max length\n",
    "preprocessor2.spacy_model.max_length = 100000000"
   ],
   "id": "947f5ad56b227472",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-11-12T21:58:04.347813Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset2 = preprocessor2.preprocess_dataset(documents_path=r'./corpus_threads_combined.txt')\n",
    "print(\"done preprocessing\")"
   ],
   "id": "92be11748305335d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"saving...\")\n",
    "dataset.save(path='./processed_dataset_2/')\n",
    "print(\"done saving\")"
   ],
   "id": "d8ce9fb8cc21e548"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T21:03:25.781015Z",
     "start_time": "2024-11-13T21:03:25.778005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# remove custom stop words that aren't caught by spacy's model\n",
    "from spacy.lang.en import stop_words\n",
    "\n",
    "stop_words = list(stop_words.STOP_WORDS)\n",
    "custom_stop_words = ['with', 'my', 'your', 'she', 'this', 'was', 'her', 'have', 'as', 'he', 'him', 'but', 'not', 'so', 'are', 'at', 'be', 'has', 'do', 'got', 'how', 'on', 'or', 'would', 'will', 'what', 'they', 'if', 'or', 'get', 'can', 'we', 'me', 'can', 'has', 'his', 'there', 'them', 'just', 'am', 'by', 'that', 'from', 'it', 'is', 'in', 'you', 'also', 'very', 'had', 'a', 'an', 'for']\n",
    "\n",
    "stop_words += custom_stop_words"
   ],
   "id": "4611233f48e0d48f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 100
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T21:09:47.557870Z",
     "start_time": "2024-11-13T21:09:47.374887Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# third pass at preprocessing, changes in max df, min df, and using custom stop words\n",
    "preprocessor3 = Preprocessing(vocabulary=None, max_features=None,\n",
    "                             remove_punctuation=True, punctuation=string.punctuation,\n",
    "                             lemmatize=True, stopword_list=custom_stop_words,\n",
    "                             min_chars=1, min_words_docs=20, num_processes=10, min_df=0.01, max_df=0.5)"
   ],
   "id": "ec04200b458245c7",
   "outputs": [],
   "execution_count": 112
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "preprocessor3.spacy_model.max_length = 100000000",
   "id": "eb9be60e0e94826a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T18:30:05.070595Z",
     "start_time": "2024-11-13T18:30:05.067701Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# check max doc length amongst the sampled datapoints\n",
    "trimmed['cleaned_message'].str.len().max()"
   ],
   "id": "36b35edbf959bfe2",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98342"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T21:10:31.654780Z",
     "start_time": "2024-11-13T21:09:51.158235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# preprocess the sampled data\n",
    "dataset_trimmed_cleaned = preprocessor3.preprocess_dataset(documents_path=r'./trimmed_cleaned.txt')\n",
    "print(\"done preprocessing\")"
   ],
   "id": "3bf075cfd162e86c",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9611/9611 [00:38<00:00, 251.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "created vocab\n",
      "893\n",
      "done preprocessing\n"
     ]
    }
   ],
   "execution_count": 113
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T21:02:51.696756Z",
     "start_time": "2024-11-13T21:02:51.678516Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save dataset\n",
    "print(\"saving...\")\n",
    "dataset_trimmed_cleaned.save(path='./processed_dataset_trimmed/')\n",
    "print(\"done saving\")"
   ],
   "id": "56865d496066782",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving...\n",
      "done saving\n"
     ]
    }
   ],
   "execution_count": 99
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T20:32:36.435058Z",
     "start_time": "2024-11-13T20:32:36.420882Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# NOTE THIS IS A CUSTOM LDA MODEL THAT BUILDS OFF THE OCTIS LDA - THIS ONE USES TF-IDF INSTEAD OF BAG OF WORDS\n",
    "\n",
    "from octis.models.model import AbstractModel\n",
    "import numpy as np\n",
    "from gensim.models import ldamodel\n",
    "import gensim.corpora as corpora\n",
    "import octis.configuration.citations as citations\n",
    "import octis.configuration.defaults as defaults\n",
    "from gensim.models import TfidfModel\n",
    "\n",
    "\n",
    "class LDA(AbstractModel):\n",
    "\n",
    "    id2word = None\n",
    "    id_corpus = None\n",
    "    use_partitions = True\n",
    "    update_with_test = False\n",
    "\n",
    "    def __init__(\n",
    "        self, num_topics=100, distributed=False, chunksize=2000,\n",
    "        passes=1, update_every=1, alpha=\"symmetric\", eta=None, decay=0.5,\n",
    "        offset=1.0, eval_every=10, iterations=50, gamma_threshold=0.001,\n",
    "            random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize LDA model\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        num_topics (int, optional) – The number of requested latent topics to\n",
    "        be extracted from the training corpus.\n",
    "\n",
    "        distributed (bool, optional) – Whether distributed computing should be\n",
    "        used to accelerate training.\n",
    "\n",
    "        chunksize (int, optional) – Number of documents to be used in each\n",
    "        training chunk.\n",
    "\n",
    "        passes (int, optional) – Number of passes through the corpus during\n",
    "        training.\n",
    "\n",
    "        update_every (int, optional) – Number of documents to be iterated\n",
    "        through for each update. Set to 0 for batch learning, > 1 for\n",
    "        online iterative learning.\n",
    "\n",
    "        alpha ({numpy.ndarray, str}, optional) – Can be set to an 1D array of\n",
    "        length equal to the number of expected topics that expresses our\n",
    "        a-priori belief for the each topics’ probability. Alternatively\n",
    "        default prior selecting strategies can be employed by supplying\n",
    "        a string:\n",
    "\n",
    "            ’asymmetric’: Uses a fixed normalized asymmetric prior of\n",
    "            1.0 / topicno.\n",
    "\n",
    "            ’auto’: Learns an asymmetric prior from the corpus\n",
    "            (not available if distributed==True).\n",
    "\n",
    "        eta ({float, np.array, str}, optional) – A-priori belief on word\n",
    "        probability, this can be:\n",
    "\n",
    "            scalar for a symmetric prior over topic/word probability,\n",
    "\n",
    "            vector of length num_words to denote an asymmetric user defined\n",
    "            probability for each word,\n",
    "\n",
    "            matrix of shape (num_topics, num_words) to assign a probability\n",
    "            for each word-topic combination,\n",
    "\n",
    "            the string ‘auto’ to learn the asymmetric prior from the data.\n",
    "\n",
    "        decay (float, optional) – A number between (0.5, 1] to weight what\n",
    "        percentage of the previous lambda value is forgotten when each new\n",
    "        document is examined.\n",
    "\n",
    "        offset (float, optional) – Hyper-parameter that controls how much\n",
    "        we will slow down the first steps the first few iterations.\n",
    "\n",
    "        eval_every (int, optional) – Log perplexity is estimated every\n",
    "        that many updates. Setting this to one slows down training by ~2x.\n",
    "\n",
    "        iterations (int, optional) – Maximum number of iterations through the\n",
    "        corpus when inferring the topic distribution of a corpus.\n",
    "\n",
    "        gamma_threshold (float, optional) – Minimum change in the value of the\n",
    "        gamma parameters to continue iterating.\n",
    "\n",
    "        random_state ({np.random.RandomState, int}, optional) – Either a\n",
    "        randomState object or a seed to generate one.s\n",
    "        Useful for reproducibility.\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hyperparameters = dict()\n",
    "        self.hyperparameters[\"num_topics\"] = num_topics\n",
    "        self.hyperparameters[\"distributed\"] = distributed\n",
    "        self.hyperparameters[\"chunksize\"] = chunksize\n",
    "        self.hyperparameters[\"passes\"] = passes\n",
    "        self.hyperparameters[\"update_every\"] = update_every\n",
    "        self.hyperparameters[\"alpha\"] = alpha\n",
    "        self.hyperparameters[\"eta\"] = eta\n",
    "        self.hyperparameters[\"decay\"] = decay\n",
    "        self.hyperparameters[\"offset\"] = offset\n",
    "        self.hyperparameters[\"eval_every\"] = eval_every\n",
    "        self.hyperparameters[\"iterations\"] = iterations\n",
    "        self.hyperparameters[\"gamma_threshold\"] = gamma_threshold\n",
    "        self.hyperparameters[\"random_state\"] = random_state\n",
    "\n",
    "    def info(self):\n",
    "        \"\"\"\n",
    "        Returns model informations\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"citation\": citations.models_LDA,\n",
    "            \"name\": \"LDA, Latent Dirichlet Allocation\"\n",
    "        }\n",
    "\n",
    "    def hyperparameters_info(self):\n",
    "        \"\"\"\n",
    "        Returns hyperparameters informations\n",
    "        \"\"\"\n",
    "        return defaults.LDA_hyperparameters_info\n",
    "\n",
    "    def set_hyperparameters(self, **kwargs):\n",
    "        \"\"\"\n",
    "        Set model hyperparameters\n",
    "        \"\"\"\n",
    "        super().set_hyperparameters(**kwargs)\n",
    "        # Allow alpha to be a float in case of symmetric alpha\n",
    "        if \"alpha\" in kwargs:\n",
    "            if isinstance(kwargs[\"alpha\"], float):\n",
    "                self.hyperparameters[\"alpha\"] = [\n",
    "                    kwargs[\"alpha\"]\n",
    "                ] * self.hyperparameters[\"num_topics\"]\n",
    "\n",
    "    def partitioning(self, use_partitions, update_with_test=False):\n",
    "        \"\"\"\n",
    "        Handle the partitioning system to use and reset the model to perform\n",
    "        new evaluations\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        use_partitions: True if train/set partitioning is needed, False\n",
    "                        otherwise\n",
    "        update_with_test: True if the model should be updated with the test set,\n",
    "                          False otherwise\n",
    "        \"\"\"\n",
    "        self.use_partitions = use_partitions\n",
    "        self.update_with_test = update_with_test\n",
    "        self.id2word = None\n",
    "        self.id_corpus = None\n",
    "\n",
    "    def train_model(self, dataset, hyperparams=None, top_words=10):\n",
    "        \"\"\"\n",
    "        Train the model and return output\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        dataset : dataset to use to build the model\n",
    "        hyperparams : hyperparameters to build the model\n",
    "        top_words : if greater than 0 returns the most significant words for\n",
    "                    each topic in the output (Default True)\n",
    "        Returns\n",
    "        -------\n",
    "        result : dictionary with up to 3 entries,\n",
    "                 'topics', 'topic-word-matrix' and\n",
    "                 'topic-document-matrix'\n",
    "        \"\"\"\n",
    "        if hyperparams is None:\n",
    "            hyperparams = {}\n",
    "\n",
    "        if self.use_partitions:\n",
    "            train_corpus, test_corpus = dataset.get_partitioned_corpus(\n",
    "                use_validation=False)\n",
    "        else:\n",
    "            train_corpus = dataset.get_corpus()\n",
    "\n",
    "        if self.id2word is None:\n",
    "            self.id2word = corpora.Dictionary(dataset.get_corpus())\n",
    "\n",
    "        if self.id_corpus is None:\n",
    "            self.id_corpus = [self.id2word.doc2bow(document)\n",
    "                              for document in train_corpus]\n",
    "\n",
    "        if \"num_topics\" not in hyperparams:\n",
    "            hyperparams[\"num_topics\"] = self.hyperparameters[\"num_topics\"]\n",
    "\n",
    "        # Allow alpha to be a float in case of symmetric alpha\n",
    "        if \"alpha\" in hyperparams:\n",
    "            if isinstance(hyperparams[\"alpha\"], float):\n",
    "                hyperparams[\"alpha\"] = [\n",
    "                    hyperparams[\"alpha\"]\n",
    "                ] * hyperparams[\"num_topics\"]\n",
    "\n",
    "        #### changes #####\n",
    "        print(\"using tf-idf\")\n",
    "        tfidf_model = TfidfModel(self.id_corpus)\n",
    "        tfidf_corpus = tfidf_model[self.id_corpus]\n",
    "        hyperparams[\"corpus\"] = tfidf_corpus\n",
    "        \n",
    "        \n",
    "        # hyperparams[\"corpus\"] = self.id_corpus\n",
    "        \n",
    "        hyperparams[\"id2word\"] = self.id2word\n",
    "        self.hyperparameters.update(hyperparams)\n",
    "\n",
    "        self.trained_model = ldamodel.LdaModel(**self.hyperparameters)\n",
    "\n",
    "        result = {}\n",
    "\n",
    "        result[\"topic-word-matrix\"] = self.trained_model.get_topics()\n",
    "\n",
    "        if top_words > 0:\n",
    "            topics_output = []\n",
    "            for topic in result[\"topic-word-matrix\"]:\n",
    "                top_k = np.argsort(topic)[-top_words:]\n",
    "                top_k_words = list(reversed([self.id2word[i] for i in top_k]))\n",
    "                topics_output.append(top_k_words)\n",
    "            result[\"topics\"] = topics_output\n",
    "\n",
    "        result[\"topic-document-matrix\"] = self._get_topic_document_matrix()\n",
    "\n",
    "        if self.use_partitions:\n",
    "            new_corpus = [self.id2word.doc2bow(\n",
    "                document) for document in test_corpus]\n",
    "            if self.update_with_test:\n",
    "                self.trained_model.update(new_corpus)\n",
    "                self.id_corpus.extend(new_corpus)\n",
    "\n",
    "                result[\"test-topic-word-matrix\"] = (\n",
    "                    self.trained_model.get_topics())\n",
    "\n",
    "                if top_words > 0:\n",
    "                    topics_output = []\n",
    "                    for topic in result[\"test-topic-word-matrix\"]:\n",
    "                        top_k = np.argsort(topic)[-top_words:]\n",
    "                        top_k_words = list(\n",
    "                            reversed([self.id2word[i] for i in top_k]))\n",
    "                        topics_output.append(top_k_words)\n",
    "                    result[\"test-topics\"] = topics_output\n",
    "\n",
    "                result[\"test-topic-document-matrix\"] = (\n",
    "                    self._get_topic_document_matrix())\n",
    "\n",
    "            else:\n",
    "                test_document_topic_matrix = []\n",
    "                for document in new_corpus:\n",
    "                    document_topics_tuples = self.trained_model[document]\n",
    "                    document_topics = np.zeros(\n",
    "                        self.hyperparameters[\"num_topics\"])\n",
    "                    for single_tuple in document_topics_tuples:\n",
    "                        document_topics[single_tuple[0]] = single_tuple[1]\n",
    "\n",
    "                    test_document_topic_matrix.append(document_topics)\n",
    "                result[\"test-topic-document-matrix\"] = np.array(\n",
    "                    test_document_topic_matrix).transpose()\n",
    "        return result\n",
    "\n",
    "    def _get_topics_words(self, topk):\n",
    "        \"\"\"\n",
    "        Return the most significative words for each topic.\n",
    "        \"\"\"\n",
    "        topic_terms = []\n",
    "        for i in range(self.hyperparameters[\"num_topics\"]):\n",
    "            topic_words_list = []\n",
    "            for word_tuple in self.trained_model.get_topic_terms(i, topk):\n",
    "                topic_words_list.append(self.id2word[word_tuple[0]])\n",
    "            topic_terms.append(topic_words_list)\n",
    "        return topic_terms\n",
    "\n",
    "    def _get_topic_document_matrix(self):\n",
    "        \"\"\"\n",
    "        Return the topic representation of the\n",
    "        corpus\n",
    "        \"\"\"\n",
    "        doc_topic_tuples = []\n",
    "        for document in self.id_corpus:\n",
    "            doc_topic_tuples.append(\n",
    "                self.trained_model.get_document_topics(document,\n",
    "                                                       minimum_probability=0))\n",
    "\n",
    "        topic_document = np.zeros((\n",
    "            self.hyperparameters[\"num_topics\"],\n",
    "            len(doc_topic_tuples)))\n",
    "\n",
    "        for ndoc in range(len(doc_topic_tuples)):\n",
    "            document = doc_topic_tuples[ndoc]\n",
    "            for topic_tuple in document:\n",
    "                topic_document[topic_tuple[0]][ndoc] = topic_tuple[1]\n",
    "        return topic_document"
   ],
   "id": "36477ea42ae04637",
   "outputs": [],
   "execution_count": 82
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T21:23:17.755422Z",
     "start_time": "2024-11-13T21:23:16.667418Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# train custom model on sampled data\n",
    "\n",
    "model_trim = LDA(num_topics=6)\n",
    "\n",
    "\n",
    "print(\"training lda...\")\n",
    "\n",
    "# Train the model using default partitioning choice\n",
    "output = model_trim.train_model(dataset_trimmed_cleaned, top_words=50)\n",
    "\n",
    "print(\"done training\")\n",
    "\n",
    "print(*list(output.keys()), sep=\"\\n\") # Print the output identifiers\n",
    "\n",
    "for t in output['topics'][:20]:\n",
    "  print(\" \".join(t))"
   ],
   "id": "4da160ebb7ad3f9c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training lda...\n",
      "using tf-idf\n",
      "done training\n",
      "topic-word-matrix\n",
      "topics\n",
      "topic-document-matrix\n",
      "test-topic-document-matrix\n",
      "care nursing home memory people sum dementia eat cares go some time then one being into meditation say funding help want when no other which staff their option could out homes ca mother who difficult way needs think etc been give any work now day term stages someone need something\n",
      "care home think sum all no could time been up who about homes many family feel years one help did only some day house dementia does our now any room like someone really call after needs which own mother know more when people live other dad move right were may\n",
      "sum dementia home all care dad time could about know things when help been now up too husband go did well here day may were then see some out one feel make more does think people should said best really much memory our who mother want years no still like\n",
      "did all who home now care no sum said when go us because only up know over out been one time after like going dad thing think our feel tea into dementia life years then people things some still night last husband own happening mind good about does day using\n",
      "some more care help been forms all about sum wife could any may days years day really home dementia out know time said dad up need after nothing good when now no think going feel see one things back who love should year way into person people happy much take\n",
      "care up then bed sum home things pain been used now go because hospital bad over all like seems back know out did day into see house too about when more were think today still food days does put much 5 morning which getting some week take than time said\n"
     ]
    }
   ],
   "execution_count": 134
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T21:05:37.003954Z",
     "start_time": "2024-11-13T21:05:37.002088Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print results\n",
    "for t in output['topics']:\n",
    "  print(\" \".join(t))\n",
    "  print(\"\\n\")"
   ],
   "id": "4209ac35d9babf71",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "needs home may point sum pain out were care which comes about no think been mother need go more sorry sums any say said know keep off too much accept\n",
      "\n",
      "\n",
      "sum phone help 1 some after does think always advice down anyone go did night may dad home like care small taking no up room time given now give able\n",
      "\n",
      "\n",
      "night any bed husband dementia years which dad vascular been here why time could back does out day who put hospital support no memory really after now diagnosed go around\n",
      "\n",
      "\n",
      "care home about ca no then sister time said did one when our their help keep down years been much week look into things all see dad because carer though\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 108
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T21:08:12.397800Z",
     "start_time": "2024-11-13T21:08:12.394189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# TWO FUNCTIONS FOR POST-PROCESSING\n",
    "\n",
    "from typing import List\n",
    "\n",
    "# take the outputted lists and return only words that are unique to their respective lists\n",
    "def unique_words(lists: List[List[str]]) -> List[List[str]]:\n",
    "    # Convert each sublist to a set for easier manipulation\n",
    "    sets = [set(sublist) for sublist in lists]\n",
    "    unique_lists = []\n",
    "\n",
    "    for i, word_set in enumerate(sets):\n",
    "        # Calculate the union of all sets except the current one\n",
    "        other_words = set().union(*[s for j, s in enumerate(sets) if j != i])\n",
    "        # Find words unique to the current set\n",
    "        unique_lists.append(list(word_set - other_words))\n",
    "\n",
    "    return unique_lists\n",
    "\n",
    "\n",
    "# take outputted words and keep only words that show up in less than <threshold> percent of topics\n",
    "def unique_words_threshold(lists: List[List[str]], threshold: float) -> List[List[str]]:\n",
    "     # Total number of lists\n",
    "    num_lists = len(lists)\n",
    "    # Calculate the threshold count based on the percentage\n",
    "    max_allowed_count = int(threshold * num_lists)\n",
    "     \n",
    "     \n",
    "    # Convert each sublist to a set for easier manipulation\n",
    "    sets = [set(sublist) for sublist in lists]\n",
    "    unique_lists = []\n",
    "    \n",
    "    for i, word_set in enumerate(sets):\n",
    "        new_set = []\n",
    "        for word in word_set:\n",
    "            other_counts = 0\n",
    "            for j, other_set in enumerate(sets):\n",
    "                if word in other_set and i != j:\n",
    "                    other_counts += 1\n",
    "        \n",
    "            if other_counts <= max_allowed_count:\n",
    "                new_set.append(word)\n",
    "        unique_lists.append(new_set)\n",
    "\n",
    "    return unique_lists"
   ],
   "id": "9f3cf7529dbc545f",
   "outputs": [],
   "execution_count": 109
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-13T21:10:49.422921Z",
     "start_time": "2024-11-13T21:10:49.420325Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print unique words\n",
    "for l in unique_words(output['topics']):\n",
    "    print(\" \".join(l))\n",
    "    print(\"\\n\")"
   ],
   "id": "e5e7f6fc62606339",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "need cares needs own wife see\n",
      "\n",
      "\n",
      "someone call weeks months hospital last 10 tea again phone pain taking should told agency after\n",
      "\n",
      "\n",
      "times really being memory than\n",
      "\n",
      "\n",
      "where house never off thought down car 3 made shone first even week using family seems ca\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 115
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T16:08:08.327328Z",
     "start_time": "2024-11-14T16:08:08.323368Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print 20 percent threshold results\n",
    "unique_threshold_20 = unique_words_threshold(output['topics'], 0.2)\n",
    "\n",
    "for l in unique_threshold_20:\n",
    "    print(\" \".join(l))\n",
    "    print(\"\\n\")"
   ],
   "id": "7a068f7d913f258e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "their homes etc eat being option way staff say stages someone need want memory ca something cares needs term meditation work give other funding nursing difficult\n",
      "\n",
      "\n",
      "homes many call right someone room house move live family needs other own only\n",
      "\n",
      "\n",
      "best well too want memory husband should here make\n",
      "\n",
      "\n",
      "happening because over tea last mind going using night thing life husband us own only good\n",
      "\n",
      "\n",
      "way forms need nothing going happy take should love back days person wife good year\n",
      "\n",
      "\n",
      "5 week bed seems house too because over take food bad getting used morning back hospital days pain put today than\n",
      "\n",
      "\n"
     ]
    }
   ],
   "execution_count": 149
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
